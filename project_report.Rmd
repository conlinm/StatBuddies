---
title: "NHANES Data Analysis Project"
author: "STAT 420, Summer 2023, Preeti Agrawal, Thimira Bandara, Michael Conlin, Constatin Kappel"
date: "2023-07-26"
output:
  pdf_document: default
  html_document: 
    theme: readable
    toc: yes
urlcolor: cyan
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
```

# Title

## Introduction

## Methods

### Data Import

```{r eval=FALSE}
if (!require(NHANES)) {
  install.packages("NHANES", quiet = TRUE)  
}
```

```{r}
library(NHANES)
# head(NHANES)
```
### Initial Variable Selection

#### Rule out variables by reasoning or by exploratory analysis

We have chosen `BMI` (Body mass index (weight/height2 in kg/m2)) as our response variable. In NHANES, this data is reported for participants aged 2 years or older, so we will focus on those participants for our analysis. Provided below are all the variables in NHANES, along with our response variable `BMI`.

```{r}
sort(names(NHANES)) # alphabetic order
```

We will add all omitted variables to a data frame `df_exclude`. The variables we would like to use as predictors will be kept in a dataframe `df_keep`. The following is our reasoning for ruling out or keeping certain variables as predictors:

1. Some predictors can be ruled out right away. Our response variable is `BMI`, so we should not use body `Weight` or `Height` as predictors, because `BMI` is calculated by dividing the `Weight` by `Height`.  

2. The next group of predictors seems very closely related either by name or logic deduction, for example, age related variables such as `Age`, `AgeDecade`, `AgeMonths`. Let's quickly double-check if they are linearly related: 

```{r, fig.asp={1}}
pairs(subset(NHANES, select = c('Age', 'AgeDecade', 'AgeMonths',
                                'AgeFirstMarij', 'AgeRegMarij')))
```

`Age`, `AgeDecade` and `AgeMonth` are clearly collinear, so we will only keep `Age`. Likewise, both variables for Marijuana use appear collinear, so we keep only one, say `AgeRegMarij` and we may decide to drop it later if it is not useful. 

3. Now let's check for collinearity between different blood pressure related variables: 

```{r, fig.asp=1}
to_test = c("BPDia1", "BPDia2", "BPDia3", "BPDiaAve", "BPSys1", "BPSys2", "BPSys3", "BPSysAve" )
pairs(subset(NHANES, select=to_test))
```

The blood pressure variables fall into two groups: diastolic and systolic blood pressure readings. We would expect there to be strongly collinearity within each group, which is the case. So, we only keep the average in each group `BPDiaAve` and `BPSysAve`.

#### Refer to the Appendix for the `pairs()` plots assessment of the following variables' collinearity:

4. Let's check all variables related to alcohol: We again performed a `pairs()` plot to visualize possible collinearity, and this graph is in the Appendix. Collinearity is not as clear in this case, but we believe one predictor related to alcohol consumption may be sufficient. We will keep `AlcoholYear`. 

5. Let's now investigate the collinearity of other drug-related variables: Most of these predictors are categorical, so collinearity cannot be seen, except for `SmokeAge` and `AgeRegMarij`. The latter makes sense as this drug is usually consumed via smoking. We can thus use one as a proxy for the other. (Note: `AgeRegMarij` was in the age related group above as well and we kept it). Let's keep `SmokeNow` and `HardDrugs` as proxies for drug abuse and its potential effect on BMI. 

6. Next, let's investigate a few life-style variables related to being physically active or the opposite thereof, screen time: Due to the nature of these variables being categorical, a clear picture of collinearity is not observable. Let's keep half of these parameters for now, which are the ones with a bit denser levels, `PhysActiveDays`, `TVHrsDay`, `CompHrsDay`. 

7. Now let's look into some other health related variables, such as cholesterol and diabetes related predictors:
`DirectChol` and `TotChol` appear to be collinear, let's keep `TotChol`. Out of the diabetes related ones, we keep `Diabetes`.

8. Let's analyze more health related variables, such as those related to urine volume and flow below:
Urine volume and urine flow appear collinear. Moreover, there might be collinearity between the first and second urine measurement, respectively. Let's keep `UrineVol1` for now. 

9. Next we analyze a somewhat heterogenic group of variables related to health or mental health. For example, somebody who is depressed might show little interest in doing things. Again, collinearity is not easy to spot in categorical variables. Let's pick `LittleInterest` as a mild form of mental health issue which might lead to little physical activity and obesity, and `HealthGen` as a general health rating. 

10. We decided to keep `Poverty` which is a ratio of family income to poverty guidelines, and drop `HHIncomeMid` and `HHIncome`, as they both capture similar information to what the `Poverty` variable captures. Similarly, we chose to keep `Race1` instead of `Race3` as they both capture similar information, and `Race1` has more data compared to `Race3`.

11. Finally, let's add `Poverty`, `SleepHrsNight`, `Gender`, `Race1`, `Education`, and `MartialStatus` as we believe they can have an effect on BMI, and we do not suspect collinearity.  

```{r }
#Setting up the data frames with the variables we will be excluding and keeping for model building

df_exclude = data.frame(predictor = c('Weight', 'Height', 'Age1stBaby', 'AgeDecade', 'AgeMonth',
                                      'AgeRegMarij', 'Alcohol12PlusYr', 'AlcoholDay', 'Smoke100',
                                      'SmokeAge', 'Marijuana', 'RegularMarij', "BPDia1", "BPDia2",
                                      "BPDia3", "BPSys1", "BPSys2", "BPSys3", 'PhysActive', 
                                      'TVHrsDayChild', 'CompHrsDayChild', 'DirectChol', 
                                      'DiabetesAge', "UrineFlow1", "UrineVol2", "UrineFlow2", 
                                      "DaysPhysHlthBad", "DaysMentHlthBad", "Depressed", "Race3",
                                      "nPregnancies"), 
reason_to_omit = c('linear dependence with BMI','linear dependence with BMI', 'specific by Gender',
                   'collinear with Age', 'collinear with Age', 
                   'redundant with Marijuana', 'more sparse than AlcoholYear', 'redundant with
                   AlcoholYear', 'redundant with SmokeNow', 'collinear with AgeRegMarij', 
                   'redundant with AgeRegMarij, the two might be swapped', 'redundant with Marijuana',
                   'collinear with other blood pressure predictors', 'collinear with other blood 
                   pressure predictors', 'collinear with other blood pressure predictors', 'collinear
                   with other blood pressure predictors', 'collinear with other blood pressure 
                   predictors', 'collinear with other blood pressure predictors', 'Redundant with
                   PhysActiveDays', 'redundant with TVHrsDay', 'redundant with CompHrsDay', 'collinear
                   with TotChol', 'redundant with Diabetes', 'collinear with UrineVol1', 'collinear 
                   with UrineVol1','collinear with UrineVol1', 'redundant with HealthGen', 'redundant
                   with HealthGen','redundant with HealthGen', 'redundant with Race1', 'specific by 
                   Gender'))

df_keep = data.frame(predictor = c('SurveyYr', 'Age', 'AlcoholYear', 'Marijuana', 'SmokeNow', 'HardDrugs', 
                                   'BPDiaAve', 'BPSysAve', 'PhysActiveDays', 'TVHrsDay', 
                                   'CompHrsDay', 'TotChol', 'Diabetes', 'UrineVol1', 'HealthGen',
                                   'LittleInterest', 'Poverty', 'SleepHrsNight', 'Gender', 
                                   'Race1', 'Education', 'MaritalStatus' ))

opts <- options(knitr.kable.NA = "")
knitr::kable(list(df_keep[1:11,], df_keep[12:22,]), caption = "Initial Predictors Selected", 
             col.names = "Predictor", booktabs = TRUE)
```

Next, let's build a dataset `nhanes_select` using just the above `df_keep` variables.

Furthermore, the NHANES dataset has data for 2 survey years: 2009-10 and 2011-12. There are certain variables, such as TVHrsDay and CompHrsDay which are only present within the later time period (2011_2012). To eliminate this large missing value problem, we will further filter our dataset down by the more recent year, 2011_12.

```{r}
nhanes_select = subset(NHANES, select =c(df_keep$predictor, "BMI"))
nhanes_select = nhanes_select[nhanes_select$SurveyYr == '2011_12', ] #filtering by year
nhanes_select = subset(nhanes_select, select = -c(SurveyYr)) #removing SurveyYr as a column for model building
```
The resulting dataset, after the initial variable selection above, consists of `r nrow(nhanes_select)` observations (rows) and `r ncol(nhanes_select)` variables (columns) including `BMI` and the chosen predictors.


**Convert Categorical Variables into Factor Variables**

We will now convert the categorical predictors into factors.

```{r}
nhanes_select$Marijuana = as.factor(nhanes_select$Marijuana)
nhanes_select$SmokeNow = as.factor(nhanes_select$SmokeNow)
nhanes_select$HardDrugs = as.factor(nhanes_select$HardDrugs)
nhanes_select$Diabetes = as.factor(nhanes_select$Diabetes)
nhanes_select$TVHrsDay = as.factor(nhanes_select$TVHrsDay)
nhanes_select$CompHrsDay = as.factor(nhanes_select$CompHrsDay)
nhanes_select$HealthGen = as.factor(nhanes_select$HealthGen)
nhanes_select$LittleInterest = as.factor(nhanes_select$LittleInterest)
nhanes_select$Gender = as.factor(nhanes_select$Gender)
nhanes_select$Race1 = as.factor(nhanes_select$Race1)
nhanes_select$Education = as.factor(nhanes_select$Education)
nhanes_select$MaritalStatus = as.factor(nhanes_select$MaritalStatus)
```

#### Address Missing Values

It would be helpful to have a dataset which is devoid of NAs (missing values) before we conduct our regression analysis. First let's get a quick idea of how many missing values are present in our initial dataset. 

**Identify which variables have majority Nan values**

```{r}
library(tidyverse, quietly = TRUE)

# Count the NA values in each column
na_counts = colSums(is.na(nhanes_select))

# Calculate the percentage of NA values in each column
total_rows = nrow(nhanes_select)
na_percentage = (na_counts / total_rows) * 100

# Create a data frame to store the results
na_summary = data.frame(Column = names(na_counts), NA_Count = na_counts, NA_Percentage = na_percentage)
na_summary = na_summary %>%
  arrange(desc(NA_Percentage))

# Print the summary
print(na_summary)

```

The table above is sorted according to NA percentage in descending order. The top 5 predictors as far as NAs are concerned are: `SmokeNow`, `PhysActiveDays`, `TVHrsDay`, `CompHrsDay` and `Marijuana`. Half of all predictors have greater than 25% missing values. If we eliminated all rows with any missing value, we would be left with only `r nrow(na.omit(nhanes_select))`, which is not enough observations to be meaningful. We cannot simply proceed using this data, as any regression tools we will use will need to eliminate many observations in order to proceed with the statistical calculations. It would also be inappropriate to simply eliminate these observations, although this was previously the standard approach. Eliminating this many observations would bring into question how well our study models represent the underlying population. Interpretation of our results would become more difficult, and suspicious of selective observation elimination introducing bias. This data was also costly to produce - we prefer to not simply cast it aside. We therefore decided to perform data imputation for the missing data. 

Data imputation involves the substitution of missing data with a different value. Although there are simple methods of replacing missing values with the mean or median of the variable in question, the most robust method is multiple imputation. Multiple imputation involves the generation of multiple complete datasets by replacing the missing values with data values which are modeled for each missing entry, from a plausible distribution. The imputation process can use a variety of methods for computing the imputed values, depending upon the underlying distribution of the observed values, and the relationship of those observed values and the other variables in the observation. Once the multiple complete datasets are generated, any analysis can be performed (such as linear regression) and the results of each analyses are pooled into one set of results.

We will perform the multiple imputation process with the `mice` package below. More information regarding the `mice` package can be read at the book website [Flexible Imputation of Missing Data](https://stefvanbuuren.name/fimd/)

### Data Imputation with the `mice` Package

```{r eval=FALSE}
if (!require(mice)) {
  install.packages("mice", quiet = TRUE)  
}
```

Here we will perform the imputation. Given the size of the data, this will take a bit of processing time.
First we will remove the observations where there is no entry for BMI as there are only `r sum(is.na(nhanes_select$BMI))` such observations, to avoid imputation of our response variable.


```{r}
library(mice, quietly = TRUE)
# remove the rows which have NAs for BMI
nhanes_imp = nhanes_select[!is.na(nhanes_select$BMI), ]

# perform the multiple imputation (5 datasets)
imp = mice(nhanes_imp, seed = 420, m = 5, print = FALSE)

```

See Appendix for density plots comparing the imputed and observed values.

### Initial Model Building and Diagnostics

Now that imputation is complete to address the missing data, we will build a complete additive model, to allow an initial diagnostic evaluation.

```{r}
# perform the linear regression with each of the 5 imputed datasets
fit_add <- with(imp, lm(BMI ~  Age +  AlcoholYear + Marijuana + SmokeNow +
  HardDrugs + BPDiaAve + BPSysAve + PhysActiveDays + TVHrsDay + CompHrsDay +
  TotChol +  Diabetes +  UrineVol1 +  HealthGen +  LittleInterest + Poverty +
  SleepHrsNight +  Gender +  Race1 +  Education +  MaritalStatus))

summary(fit_add$analyses[[1]]) #summary of the 1st imputed dataset
```

We will next construct a dataframe of all of our 5 imputed datasets, with the additional values
added of columns `.imp` for the imputation number, and `.i` for the observation number within that imputation.

```{r}
imp_df = mice::complete(imp, action = "long")
```

#### Collinearity

When we built the additive model above, a few parameters had large p-values. Let's check the variance inflation factors for all the predictors in this model, to see if there is any effect of collinearity on the variance of our regression estimates.  

```{r}
library(car)
car::vif(fit_add$analyses[[1]])
```

None of the variable appear to have a large (>5) variance inflation factor which is good to see.  

#### Variance and Normality Assessment

Now let's do some tests on this model to identify any potential issues.

```{r}
library(lmtest)
if (!require(nortest)) {
  install.packages("nortest", quiet = TRUE)  
}
# install.packages("nortest", quiet = TRUE)
library(nortest, quietly = TRUE)

### First, let's define some functions ###

# Function to calculate the LOOCVRMSE
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

# model diagnostics
model_diagnostics = function(fit){
  fit_summary <- data.frame(bptest_p = rep(0,5), ad_test = rep(0,5))
  for (i in 1:5){
    fit_summary$bptest_p[i] = unname(bptest(fit$analyses[[i]])$p.value)
    ad.test(residuals(fit$analyses[[i]]))$p.value 
  }
  knitr::kable(fit_summary, col.names = c("BP Test", "AD Test"))
}

# cooks distance to check for influential observations
cooks_function = function(fit){
  cook_thresh = 4 * cooks.distance(fit$analyses[[1]]) / nrow(imp)
  sum(cooks.distance(fit$analyses[[1]]) > cook_thresh)
}
 
# model assessments
model_assess = function(fit){
  fit_summary <- data.frame(adj_r_squared = rep(0,5), loocv_rmse = rep(0,5))
  for (i in 1:5){
    fit_summary$adj_r_squared[i] = summary(fit$analyses[[i]])$adj
    fit_summary$loocv_rmse[i] = calc_loocv_rmse(fit$analyses[[i]]) 
  }
  knitr::kable(fit_summary, col.names = c("Adj. R-Squared", "LOOCV-RMSE"))
}
```

```{r, fig.asp=1}
#Fitted versus Residuals Plot for the 1st imputed dataset model
plot(fitted(fit_add$analyses[[1]]), resid(fit_add$analyses[[1]]), col = "darkblue", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals Plot")
abline(h=0,col = "darkorange")
```

The Fitted versus Residuals plot reveals deviation from homoscedasticity (constant variance). 

```{r, fig.asp=1}
#Normal Q-Q Plot for the 1st imputed dataset model
qqnorm(resid(fit_add$analyses[[1]]), col = "dodgerblue")
qqline(resid(fit_add$analyses[[1]]), col = "darkorange")
```

The Q-Q-Plot also shows deviations from normality. 

Let's now look at the p-values from the AD Test for normality, and the Breusch-Pagan Test for Homoscedasticity.

```{r}
model_diagnostics(fit_add)
```
The p-values for these tests, using each of the 5 imputed dataset models, are all very low, essentially 0. So we reject the null hypothesis, calling into question, both normality and homoscedasticity. However, both of these tests are susceptible to the influence of large sample sizes, so they may be less reliable in this setting.

Because of the findings above, we will perform a variance stabilizing log transformation on the response variable (BMI), fit the model again and reassess the diagnostics.

```{r}
# perform the linear regression with each of the 5 imputed datasets
# and the log() transform of BMI
fit_add_log <- with(imp, lm(log(BMI) ~  Age +  AlcoholYear + Marijuana + SmokeNow +
  HardDrugs + BPDiaAve + BPSysAve + PhysActiveDays + TVHrsDay + CompHrsDay +
  TotChol +  Diabetes +  UrineVol1 +  HealthGen +  LittleInterest + Poverty +
  SleepHrsNight +  Gender +  Race1 +  Education +  MaritalStatus))

summary(fit_add_log$analyses[[1]])
```

```{r, fig.asp=1}
#Comparing the Fitted versus Residuals Plots of the Initial Additive model and the Log(BMI) Transformation Model
par(mfrow=c(1,2))
plot(fitted(fit_add$analyses[[1]]), resid(fit_add$analyses[[1]]), col = "darkblue", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals - BMI")
abline(h=0,col = "darkorange")
plot(fitted(fit_add_log$analyses[[1]]), resid(fit_add_log$analyses[[1]]), col = "darkblue", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals - log(BMI)")
abline(h=0,col = "darkorange")
```
The log transformation of BMI model looks much better, though still not perfect. 

Now let's look at the Q-Q plots:

```{r, fig.asp=1}
#Comparing the Normal Q-Q Plots of the Initial Additive model and the Log(BMI) Transformation Model
par(mfrow=c(1,2))
# no transformation
qqnorm(resid(fit_add$analyses[[1]]), col = "dodgerblue", main = "Normal Q-Q Plot - BMI")
qqline(resid(fit_add$analyses[[1]]), col = "darkorange")
# log transformation
qqnorm(resid(fit_add_log$analyses[[1]]), col = "dodgerblue", main = "Normal Q-Q Plot - log(BMI)")
qqline(resid(fit_add_log$analyses[[1]]), col = "darkorange")
```

Again, the log transformation of BMI results is a much better appearing QQ plot. Moving forward, we will use the log transformed BMI for our model building.

#### Model Selection

Now we use the different search procedures, backwards, forwards, and stepwise to search for models and select predictors. Notice that our 5 datasets with observed and imputed data are passed to the stepwise function using `with()` which in this case returns a `mira` object from the `mice` package.

First we will start with the additive model and perform a backward AIC model search.
```{r}
# build the stepwise workflow
scope <- list(upper = ~ Age +  AlcoholYear + Marijuana + SmokeNow +
  HardDrugs + BPDiaAve + BPSysAve + PhysActiveDays + TVHrsDay + CompHrsDay +
  TotChol +  Diabetes +  UrineVol1 +  HealthGen +  LittleInterest + Poverty +
  SleepHrsNight +  Gender +  Race1 +  Education +  MaritalStatus,
             lower = ~ 1)
expr <- expression(f1 <- lm(log(BMI) ~ 1),
                  f2 <- step(f1, scope = scope, trace = 0))
# perform the stepwise selection with each of the 5 imputed datasets
fit <- with(imp, expr)

# count the votes for variables to keep
formulas <- lapply(fit$analyses, formula)
terms <- lapply(formulas, terms)
votes <- unlist(lapply(terms, labels))
table(votes)
```

If we use the criterion of more than half of the datasets resulted in selection of a variable, we end up only dropping Education, Gender, and Marijuana. Let's compare the models using anova, leaving out variables with less than 5 votes.

```{r}
# remove HardDrugs
model_without = with(imp, lm(log(BMI) ~ Age +  AlcoholYear + SmokeNow +
  BPDiaAve + BPSysAve + PhysActiveDays + TVHrsDay + CompHrsDay +
  TotChol +  Diabetes +  UrineVol1 +  HealthGen +  LittleInterest + Poverty +
  SleepHrsNight +  Race1  +  MaritalStatus))
model_with = with(imp, lm(log(BMI) ~ Age +  AlcoholYear + SmokeNow +
  HardDrugs + BPDiaAve + BPSysAve + PhysActiveDays + TVHrsDay + CompHrsDay +
  TotChol +  Diabetes +  UrineVol1 +  HealthGen +  LittleInterest + Poverty +
  SleepHrsNight +  Race1  +  MaritalStatus))
anova(model_without, model_with)
```

This p-value is not significant, so we fail to reject the null hypothesis and we can discard `HardDrugs`.

```{r}
# remove PhysActiveDays
model_without = with(imp, lm(log(BMI) ~ Age +  AlcoholYear + SmokeNow +
  BPDiaAve + BPSysAve + TVHrsDay + CompHrsDay +
  TotChol +  Diabetes +  UrineVol1 +  HealthGen +  LittleInterest + Poverty +
  SleepHrsNight +  Race1  +  MaritalStatus))
model_with = with(imp, lm(log(BMI) ~ Age +  AlcoholYear + SmokeNow +
  BPDiaAve + BPSysAve + PhysActiveDays + TVHrsDay + CompHrsDay +
  TotChol +  Diabetes +  UrineVol1 +  HealthGen +  LittleInterest + Poverty +
  SleepHrsNight +  Race1  +  MaritalStatus))
anova(model_without, model_with)
```

Again, we fail to reject the null hypothesis based on the p-value, and can remove `PhysActiveDays`.

```{r}
# remove Poverty
model_without = with(imp, lm(log(BMI) ~  Age +  AlcoholYear + SmokeNow +
  BPDiaAve + BPSysAve + TVHrsDay + CompHrsDay +
  TotChol +  Diabetes +  UrineVol1 +  HealthGen +  LittleInterest +
  SleepHrsNight +  Race1  +  MaritalStatus))
model_with = with(imp, lm(log(BMI) ~  Age +  AlcoholYear + SmokeNow +
  BPDiaAve + BPSysAve + TVHrsDay + CompHrsDay +
  TotChol +  Diabetes +  UrineVol1 +  HealthGen +  LittleInterest + Poverty +
  SleepHrsNight +  Race1  +  MaritalStatus))
anova(model_without, model_with)
```

This p-value is again greater than 0.05, and so we will remove `Poverty` for now. 

Here is the final model of this process which we will call `fit_add_aic`

```{r}
fit_add_aic = with(imp, lm(log(BMI) ~ Age +  AlcoholYear + SmokeNow +
  BPDiaAve + BPSysAve + TVHrsDay + CompHrsDay +
  TotChol +  Diabetes +  UrineVol1 +  HealthGen +  LittleInterest +
  SleepHrsNight +  Race1  +  MaritalStatus))
summary(fit_add_aic$analyses[[1]])
```
Let's try a forward search using BIC, and see if we get a smaller model:

```{r}
# build the stepwise workflow, full scope with all predictors
scope <- list(upper = ~ Age +  AlcoholYear + Marijuana + SmokeNow +
  HardDrugs + BPDiaAve + BPSysAve + PhysActiveDays + TVHrsDay + CompHrsDay +
  TotChol +  Diabetes +  UrineVol1 +  HealthGen +  LittleInterest + Poverty +
  SleepHrsNight +  Gender +  Race1 +  Education +  MaritalStatus,
             lower = ~ 1)
expr <- expression(f1 <- lm(log(BMI) ~ 1),
                  f2 <- step(f1, scope = scope, direction = "forward", 
                             K = log(nrow(imp[["data"]])), trace = 0))
# perform the stepwise selection with each of the 5 imputed datasets
fit <- with(imp, expr)

# count the votes for variables to keep
formulas <- lapply(fit$analyses, formula)
terms <- lapply(formulas, terms)
votes <- unlist(lapply(terms, labels))
table(votes)

```

This appears to yield the same votes as the prior method, which results in the same model.

Lastly, let's try a Stepwise search in both directions using AIC.

```{r}
# build the stepwise workflow
scope <- list(upper = ~ Age +  AlcoholYear + Marijuana + SmokeNow +
  HardDrugs + BPDiaAve + BPSysAve + PhysActiveDays + TVHrsDay + CompHrsDay +
  TotChol +  Diabetes +  UrineVol1 +  HealthGen +  LittleInterest + Poverty +
  SleepHrsNight +  Gender +  Race1 +  Education +  MaritalStatus,
             lower = ~ 1)
expr <- expression(f1 <- lm(log(BMI) ~ 1),
                  f2 <- step(f1, scope = scope, direction = "both", 
                             trace = 0))
# perform the stepwise selection with each of the 5 imputed datasets
fit <- with(imp, expr)

# count the votes for variables to keep
formulas <- lapply(fit$analyses, formula)
terms <- lapply(formulas, terms)
votes <- unlist(lapply(terms, labels))
table(votes)
```

Still same results. For now, our additive model will be `fit_add_aic`

Before we move on, there are some predictors which are not significant individually, so we should check for collinearity again.


```{r}
library(car)
car::vif(fit_add_aic$analyses[[1]])
```

There appear to be no major issues with collinearity in this additive model.

#### Further Predictor Exploration 

Additionally, we want to check which variables are correlated with BMI to further investigate possible interaction terms, and determine if there are varaibles we could again consider dropping.  

```{r}
#Age +  AlcoholYear + SmokeNow +
  #BPDiaAve + BPSysAve + TVHrsDay + CompHrsDay +
 # TotChol +  Diabetes +  UrineVol1 +  HealthGen +  LittleInterest +
  #SleepHrsNight +  Race1  +  MaritalStatus


# Subset the 'nhanes' dataset using the names of numerical columns in 'numerical_data'
nhanes_numerical_subset = nhanes_select[, c('Age', 'AlcoholYear', 'BPDiaAve', 'BPSysAve', 'UrineVol1', 'SleepHrsNight', 'TotChol' ,'BMI')]  


# Calculate the correlation matrix for 'nhanes_numerical_subset'
cor_matrix  = cor(nhanes_numerical_subset, use = "complete.obs")

# Create the correlation plot for 'BMI' and other numeric variables
corrplot(cor_matrix, type = "upper", tl.cex = 0.8, tl.col = "black", tl.srt = 45)

```


```{r}

if (!require(vcd)) {
  install.packages("vcd", quiet = TRUE)  
}

library(vcd)

# Subset the 'nhanes' dataset using the names of categorical columns in 'categorical_data'
nhanes_categorical_subset = nhanes_select[, c('Marijuana', 'SmokeNow', 'HardDrugs', 'Diabetes', 'LittleInterest', 'TVHrsDay', 'CompHrsDay',  'BMI')]   

# Assuming 'nhanes_categorical_subset' already contains the 'BMI' column and categorical variables

# Perform a Chi-square test for each categorical variable
for (var in names(nhanes_categorical_subset)) {
  if (is.factor(nhanes_categorical_subset[[var]])) {
    chi_result <- chisq.test(nhanes_categorical_subset[[var]], nhanes_categorical_subset$BMI)
    print(paste("Variable:", var))
    print(chi_result)
  }
}


```

Based Upon the findings above, it seems reasonable to try removing both `UrineVol1` and `TotChol`, as there individual correlations with BMI are negligible.

First we will try removing `UrineVol1`:

```{r}
fit_add_aic_with = with(imp, lm(log(BMI) ~ Age +  AlcoholYear + SmokeNow +
  BPDiaAve + BPSysAve + TVHrsDay + CompHrsDay +
  TotChol +  Diabetes +  UrineVol1 +  HealthGen +  LittleInterest +
  SleepHrsNight +  Race1  +  MaritalStatus))
# removing UrineVol1 
fit_add_aic_without =with(imp, lm(log(BMI) ~ Age +  AlcoholYear + SmokeNow +
  BPDiaAve + BPSysAve + TVHrsDay + CompHrsDay +
  TotChol +  Diabetes +  HealthGen +  LittleInterest +
  SleepHrsNight +  Race1  +  MaritalStatus))
anova(fit_add_aic_without, fit_add_aic_with)
```

Since the p-value of the anova test comparing the models with and without `UrineVol` is greater than 0.01, we fail to reject the null hypothesis, and should remove the variable.

Now consider removing `Tchol`.

```{r}
fit_add_aic_with = with(imp, lm(log(BMI) ~ Age +  AlcoholYear + SmokeNow +
  BPDiaAve + BPSysAve + TVHrsDay + CompHrsDay +
  TotChol +  Diabetes +  HealthGen +  LittleInterest +
  SleepHrsNight +  Race1  +  MaritalStatus))
# removing TotChol
fit_add_aic_without =with(imp, lm(log(BMI) ~ Age +  AlcoholYear + SmokeNow +
  BPDiaAve + BPSysAve + TVHrsDay + CompHrsDay +
  Diabetes +  HealthGen +  LittleInterest +
  SleepHrsNight +  Race1  +  MaritalStatus))
anova(fit_add_aic_without, fit_add_aic_with)
```

Likewise, the p-value is greater than 0.01, although it is less than 0.05, so for now we can leave it.

So our **final simple additive model** is below, and we will name it `final_add`.

```{r}
final_add = with(imp, lm(log(BMI) ~ Age +  AlcoholYear + SmokeNow +
  BPDiaAve + BPSysAve + TVHrsDay + CompHrsDay +
  TotChol +  Diabetes +  HealthGen +  LittleInterest +
  SleepHrsNight +  Race1  +  MaritalStatus))
summary(final_add$analyses[[1]])
```



#### Data Interactions


To determine if interaction terms might improve our model, we will perform a backward AIC stepwise() function with added `Age` interaction terms. This seemed the most plausable and made the most sense from a domain perspective. It could be understandable for instance, that alcohol use at a young age vs an older age might have cpmbined effects upon BMI.


```{r}
# build the stepwise workflow using our fit_add_aic 
# with interaction terms added as a starting point
scope <- list(upper = ~ Age +  AlcoholYear + SmokeNow +
  BPDiaAve + BPSysAve + TVHrsDay + CompHrsDay +
  TotChol +  Diabetes + HealthGen +  LittleInterest +
  SleepHrsNight +  Race1  +  MaritalStatus +  Age:AlcoholYear +  Age:SmokeNow +  Age:BPDiaAve +  Age:BPSysAve  + Age:TVHrsDay + Age:CompHrsDay +  Age:TotChol +  Age:Diabetes +  Age:HealthGen +  Age:LittleInterest +  Age:SleepHrsNight +  Age:Race1 +  Age:MaritalStatus,
             lower = ~ 1)
expr <- expression(f1 <- lm(log(BMI) ~ 1),
                  f2 <- step(f1, scope = scope, trace = 0))
# perform the stepwise selection with each of the 5 imputed datasets
fit <- with(imp, expr)

# count the votes for variables to keep
formulas <- lapply(fit$analyses, formula)
terms <- lapply(formulas, terms)
votes <- unlist(lapply(terms, labels))
table(votes)

```

Build the model with all the above predictors with > 3 votes

```{r}
fit_int_aic = with(imp, lm(log(BMI) ~ Age +  AlcoholYear + SmokeNow +
  BPDiaAve + BPSysAve + TVHrsDay + CompHrsDay +
  Diabetes +  HealthGen +  LittleInterest +
  SleepHrsNight +  Race1  +  MaritalStatus +  Age:SmokeNow +  Age:BPDiaAve +  Age:BPSysAve  + Age:TVHrsDay  +  Age:Diabetes +   Age:HealthGen +  Age:Race1 +  Age:MaritalStatus))
summary(fit_int_aic$analyses[[1]])
```

Consider taking out `Age:TVHrsDay`

```{r}
fit_int_aic_with = with(imp, lm(log(BMI) ~  Age +  AlcoholYear + SmokeNow +
  BPDiaAve + BPSysAve + TVHrsDay + CompHrsDay +
  Diabetes +  HealthGen +  LittleInterest +
  SleepHrsNight +  Race1  +  MaritalStatus +  Age:SmokeNow +  Age:BPDiaAve +  Age:BPSysAve  + Age:TVHrsDay  +  Age:Diabetes +   Age:HealthGen +  Age:Race1 +  Age:MaritalStatus))
# removing Age:TVHrsDay
fit_int_aic_without =with(imp, lm(log(BMI) ~  Age +  AlcoholYear + SmokeNow +
  BPDiaAve + BPSysAve + TVHrsDay + CompHrsDay +
  Diabetes +  HealthGen +  LittleInterest +
  SleepHrsNight +  Race1  +  MaritalStatus +  Age:SmokeNow +  Age:BPDiaAve +  Age:BPSysAve  +  Age:Diabetes +   Age:HealthGen +  Age:Race1 +  Age:MaritalStatus))
anova(fit_int_aic_without, fit_int_aic_with)
```

The p-value is less than 0.05, so for now, we will keep `Age:TVHrsDay`.

We also notice from the model that the the p-values for `Race1` categories and their interaction terms are not significant, and the estimates are low. Perhaps we can remove Race1, and the interaction term.

```{r}
fit_int_aic_with = with(imp, lm(log(BMI) ~  Age +  AlcoholYear + SmokeNow +
  BPDiaAve + BPSysAve + TVHrsDay + CompHrsDay +
  Diabetes +  HealthGen +  LittleInterest +
  SleepHrsNight +  Race1  +  MaritalStatus +  Age:SmokeNow +  Age:BPDiaAve +  Age:BPSysAve  + Age:TVHrsDay  +  Age:Diabetes +   Age:HealthGen +  Age:Race1 +  Age:MaritalStatus))
# removing Age:Race1 and interaction
fit_int_aic_without =with(imp, lm(log(BMI) ~  Age +  AlcoholYear + SmokeNow +
  BPDiaAve + BPSysAve + TVHrsDay + CompHrsDay +
  Diabetes +  HealthGen +  LittleInterest +
  SleepHrsNight +  MaritalStatus +  Age:SmokeNow +  Age:BPDiaAve +  Age:BPSysAve  +  Age:Diabetes +   Age:HealthGen +  Age:MaritalStatus))
anova(fit_int_aic_without, fit_int_aic_with)
```

The p-value is very low, less than 0.01, so we reject the null hypothesis, and will keep `Race1` and the related interaction term.

So our final model with the addition of interactions is `final_int`, and is given below:

```{r}
final_int = with(imp, lm(log(BMI) ~  Age +  AlcoholYear + SmokeNow +
  BPDiaAve + BPSysAve + TVHrsDay + CompHrsDay +
  Diabetes +  HealthGen +  LittleInterest +
  SleepHrsNight +  Race1  +  MaritalStatus +  Age:SmokeNow +  Age:BPDiaAve +  Age:BPSysAve  + Age:TVHrsDay  +  Age:Diabetes +   Age:HealthGen +  Age:Race1 +  Age:MaritalStatus))
summary(final_int$analyses[[1]])
```

#### Data Transformations

Now we will consider if our model will benefit from any data transformations. The two numerical variables we are most suspicious of having skewed distributions are those related to blood pressure. Let's graph each of their distributions. We are using the imputed data, but only one of the 5 imputed datasets.

BPSysAve Transformation:

```{r}
par(mfrow=c(1,2))
hist(imp_df[imp_df$.imp == 1, ]$BPSysAve,
     main = "Histogram - BPSysAve",
     xlab = "BPSysAve")
hist(log(imp_df[imp_df$.imp == 1, ]$BPSysAve),
     main = "Histogram - Log(BPSysAve)",
     xlab = "Log(BPSysAve)")
```

We noticed that the non-transformed `BPSysAve` data appeared skewed right, so we applied a log transformation, and the distribution has improved

BPDiasAve Trnasformation:

```{r}
par(mfrow=c(1,2))
hist(imp_df[imp_df$.imp == 1, ]$BPDiaAve,
     main = "Histogram - BPDiaAve",
     xlab = "BPDiaAve")
hist((imp_df[imp_df$.imp == 1, ]$BPDiaAve)^2,
     main = "Histogram - BPDiaAve^2",
     xlab = "BPDiaAve^2")
```

In the case of `BPDiaAve`, we noticed that the non-transformed data appeared skewed left, so we applied a log transformation, and the distribution has improved somewhat, though it is a bit right skewed, but less skewed overall.

Now we will consider our model with the log transformed BP measures:


```{r}
final_trns = with(imp, lm(log(BMI) ~ Age +  AlcoholYear + SmokeNow +
  I(BPDiaAve^2) + log1p(BPSysAve) + TVHrsDay + CompHrsDay +
  Diabetes +  HealthGen +  LittleInterest +
  SleepHrsNight +  Race1  +  MaritalStatus +  Age:SmokeNow +  Age:I(BPDiaAve^2) +  Age:log1p(BPSysAve)  + Age:TVHrsDay +  Age:Diabetes +  Age:HealthGen +  Age:Race1 +  Age:MaritalStatus))
summary(final_trns$analyses[[1]])
```
We notice that our R-Squared is the same as without the transformed data, but this will represent our final transformation model and we will more fully compare all three models in the next section.

#### Final Models

I will fill this out further later:

final_add
final_int
final_trns

## Results

#### Outlier Assessment

```{r}
#finding outliers for each model

indexs_trns = which(cooks.distance(final_trns$analyses[[1]]) > 4 / length(cooks.distance(final_trns$analyses[[1]])))

indexs_int = which(cooks.distance(final_int$analyses[[1]]) > 4 / length(cooks.distance(final_int$analyses[[1]])))

indexs_add = which(cooks.distance(final_add$analyses[[1]]) > 4 / length(cooks.distance(final_add$analyses[[1]])))


#removing outliers for each model

imp_df_1 = imp_df[imp_df$.imp == 1, ]
im_df_1_trns_rm = imp_df_1[-indexs_trns, ]
im_df_1_int_rm = imp_df_1[-indexs_int, ]
im_df_1_add_rm = imp_df_1[-indexs_add, ]
```

```{r}
#Final_trns Model fit with outliers removed 

lm(log(BMI) ~ Age +  AlcoholYear + SmokeNow +
  I(BPDiaAve^2) + log1p(BPSysAve) + TVHrsDay + CompHrsDay +
  Diabetes +  HealthGen +  LittleInterest +
  SleepHrsNight +  Race1  +  MaritalStatus +  Age:SmokeNow +  Age:I(BPDiaAve^2) +  Age:log1p(BPSysAve)  + Age:TVHrsDay +  Age:Diabetes +  Age:HealthGen +  Age:Race1 +  Age:MaritalStatus, data = im_df_1_trns_rm)

```

```{r}
#Final_int Model fit with outliers removed

lm(log(BMI) ~  Age +  AlcoholYear + SmokeNow +
  BPDiaAve + BPSysAve + TVHrsDay + CompHrsDay +
  Diabetes +  HealthGen +  LittleInterest +
  SleepHrsNight +  Race1  +  MaritalStatus +  Age:SmokeNow +  Age:BPDiaAve +  Age:BPSysAve  + Age:TVHrsDay  +  Age:Diabetes +   Age:HealthGen +  Age:Race1 +  Age:MaritalStatus, data = im_df_1_int_rm)

```


```{r}
#Final_add Model fit with outliers removed

lm(log(BMI) ~  Age +  AlcoholYear + SmokeNow +
  BPDiaAve + BPSysAve + TVHrsDay + CompHrsDay +
  Diabetes +  HealthGen +  LittleInterest +
  SleepHrsNight +  Race1  +  MaritalStatus +  Age:SmokeNow +  Age:BPDiaAve +  Age:BPSysAve  + Age:TVHrsDay  +  Age:Diabetes +   Age:HealthGen +  Age:Race1 +  Age:MaritalStatus, data = im_df_1_add_rm)

```

### Model Diagnostics

## Discussion

## Appendix

### Variable Selection

#### Additional `pairs()` Plots for Collinearity Assessment

Alcohol related variables:

```{r, fig.asp=1}
to_test = c("Alcohol12PlusYr", "AlcoholDay", "AlcoholYear")
pairs(subset(NHANES, select = to_test))
```

Smoking and Drug related variables:

```{r, fig.asp=1}
to_test = c("SmokeNow", "Smoke100", "SmokeAge", "Marijuana", "RegularMarij", "AgeRegMarij", "HardDrugs")
pairs(subset(NHANES, select = to_test))

```

Lifestyle related variables:

```{r, fig.asp=1}
to_test = c("PhysActive", "PhysActiveDays", "TVHrsDay", "CompHrsDay", "TVHrsDayChild", "CompHrsDayChild")
pairs(subset(NHANES, select=to_test))
```

Cholesterol related variables:

```{r, fig.asp=1}
to_test = c("DirectChol", "TotChol", "Diabetes", "DiabetesAge")
pairs(subset(NHANES, select = to_test))
```

Urine related variables:

```{r, fig.asp=1}
to_test = c("UrineVol1", "UrineFlow1", "UrineVol2", "UrineFlow2")
pairs(subset(NHANES, select = to_test))
```

Mental health related variables:

```{r, fig.asp=1}
to_test = c("HealthGen", "DaysPhysHlthBad", "DaysMentHlthBad", "LittleInterest", "Depressed" )
pairs(subset(NHANES, select = to_test))
```

#### Data Imputation

```{r}
# Compare the imputed variables (red) and observed (blue)
densityplot(imp)
# summary(imp)
```

