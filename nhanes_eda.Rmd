---
title: "nhanes_eda"
author: "STAT 420, Summer 2023, Preeti Agrawal, Thimira Bandara, Michael Conlin, Constatin Kappel"
date: "2023-07-31"
output: html_document
---
```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
```


## Loading Data

Loading the NHANES data for exploration. 

```{r cars}
library(NHANES)
head(NHANES)
```

## Ruling out variables

### Ruling out by reasoning

These are all predictors in NHANES

```{r}
sort(names(NHANES)) # alphabetic order
```

All omitted predictors go into an overview table `df_exclude`. The variables to keep go into `df_keep`. 

1. Some predictors can be ruled out right away. Our response variable is `BMI`, so we should not use body `Weight` or `Height` as predictors. 
1. The next group of predictors seems very closely related either by name or logic deduction, e.g. `Age`, `AgeDecade`, `AgeMonths`. Let's quickly double-check they are linearly related: 

```{r, fig.asp={1}}
pairs(subset(NHANES, select = c('Age', 'Age1stBaby', 'AgeDecade', 'AgeMonths', 'AgeFirstMarij', 'AgeRegMarij')))
```

`Age`, `AgeDecade` and `AgeMonth` are clearly colinear, so we keep only `Age`. Likewise, both variables for Marijuana use appear colinear, so we keep only one, say `AgeRegMarij` and could decide to drop it later if not useful. We might keep `Age1stBaby`. 

3. Next up we check all variables related to alcohol:

```{r, fig.asp=1}
to_test = c("Alcohol12PlusYr", "AlcoholDay", "AlcoholYear")
pairs(subset(NHANES, select = to_test))
```

Colinearity is not as clear in this case, but one predictor related to alcohol consumption may be enough. Let's keep `AlcoholYear`. 

4. Let's now investigate the colinearity of other drug-related variables (note: `AgeRegMarij` was in the other group as well and we kept it):

```{r, fig.asp=1}
to_test = c("SmokeNow", "Smoke100", "SmokeAge", "Marijuana", "RegularMarij", "AgeRegMarij", "HardDrugs")
pairs(subset(NHANES, select = to_test))

```

Most of these predictors are categorical, so colinearity can not be seen, except for `SmokeAge` and `AgeRegMarij`. The latter makes sense as this drug is usually consumed via smoking. We can thus use one as a proxy for the other. Let's keep `SmokeNow` and `HardDrugs` as proxies for drug abuse and its potential effect on BMI. 

5. Now let's check for colinearity between different blood pressure related variables: 

```{r, fig.asp=1}
to_test = c("BPDia1", "BPDia2", "BPDia3", "BPDiaAve", "BPSys1", "BPSys2", "BPSys3", "BPSysAve" )
pairs(subset(NHANES, select=to_test))
```

The blood pressure variables fall into two groups: diastolic and systolic blood pressure readings. Obviously, the must be strongly colinear within each group, which is the case. So, we only keep the average in each group `BPDiaAve` and `BPSysAve`.

6. Next up let's investigate a few life-style variables related to being physically active or the opposite thereof, screentime:

```{r, fig.asp=1}
to_test = c("PhysActive", "PhysActiveDays", "TVHrsDay", "CompHrsDay", "TVHrsDayChild", "CompHrsDayChild")
pairs(subset(NHANES, select=to_test))
```

Due to the nature of these variables being categorical, a clear picture of colinearity is not observable. Let's keep for now half of these parameters which have a bit denser levels, `PhysActiveDays`, `TVHrsDay`, `CompHrsDay`. 

7. Now we should look into some other health related variables. Let's see for cholesterol and diabetes related predictors:

```{r, fig.asp=1}
to_test = c("DirectChol", "TotChol", "Diabetes", "DiabetesAge")
pairs(subset(NHANES, select = to_test))
```

`DirectChol` and `TotChol` appear to be colinear, let's keep `TotChol`. Of diabetes related ones we keep `Diabetes`. 

8. More health related variables, here related to urogenesis:

```{r, fig.asp=1}
to_test = c("UrineVol1", "UrineFlow1", "UrineVol2", "UrineFlow2")
pairs(subset(NHANES, select = to_test))
```

Urine volume and urine flow appear colinear. Moreover, there might be colinearity between the first and second urine measurement, respectively. Let's keep `UrineVol1` for now. 

9. Next up are a somewhat heterogenic group of variables related to health or mental health. E.g. somebody who is depressed might show little interest in doing things. 

```{r, fig.asp=1}
to_test = c("HealthGen", "DaysPhysHlthBad", "DaysMentHlthBad", "LittleInterest", "Depressed" )
pairs(subset(NHANES, select = to_test))
```

Again, colinearity is not easy to spot in categorical variables. Let's pick `LittleInterest` as a mild form of mental health issue (which might lead to little physical activity and obesity) and `HealthGen` as a general health rating. 

10. Finally, let's add `nPregnancies` as pregnancy has an effect on body mass. 

```{r }
df_exclude = data.frame(predictor = c('Weight', 'Height', 'AgeDecade', 'AgeMonth', 'AgeRegMarij', 'Alcohol12PlusYr', 'AlcoholDay', 'Smoke100', 'SmokeAge', 'Marijuana', 'RegularMarij', "BPDia1", "BPDia2", "BPDia3", "BPSys1", "BPSys2", "BPSys3", 'PhysActive', 'TVHrsDayChild', 'CompHrsDayChild', 'DirectChol', 'DiabetesAge', "UrineFlow1", "UrineVol2", "UrineFlow2", "DaysPhysHlthBad", "DaysMentHlthBad", "Depressed"), 
                        reason_to_omit = c('linear dependence with BMI','linear dependence with BMI', 'colinear with Age', 'colinear with Age', 'redundant with Marijuana', 'more sparse than AlcoholYear', 'redundant with AlcoholYear', 'redundant with SmokeNow', 'colinear with AgeRegMarij', 'redundant with AgeRegMarij, the two might be swapped', 'redundant with Marijuana', 'colinear with other blood pressure predictors', 'colinear with other blood pressure predictors', 'colinear with other blood pressure predictors', 'colinear with other blood pressure predictors', 'colinear with other blood pressure predictors', 'colinear with other blood pressure predictors', 'Redundant with PhysActiveDays', 'redundant with TVHrsDay', 'redundant with CompHrsDay', 'colinear with TotChol', 'redundant with Diabetes', 'colinear with UrineVol1', 'colinear with UrineVol1', 'colinear with UrineVol1', 'redundant with HealthGen', 'redundant with HealthGen', 'redundant with HealthGen'))
df_keep = data.frame(predictor = c('Age', 'Age1stBaby', 'AlcoholYear', 'Marijuana', 'SmokeNow', 'HardDrugs', 'BPDiaAve', 'BPSysAve', 'PhysActiveDays', 'TVHrsDay', 'CompHrsDay', 'TotChol', 'Diabetes', 'UrineVol1', 'HealthGen', 'LittleInterest', 'nPregnancies'))
#kable(df_exclude)


```

### Ruling out by exploratory analysis



```{r}

```


### Selected and reduced data sets

The above selection process based on reasoning and exploration reduces the number of predictors in NHANES from `r ncol(NHANES) - 3` to `r nrow(df_keep)`. 

Let's build a dataset `nhanes_select` using just those variables. 

```{r}
nhanes_select = subset(NHANES, select =c(df_keep$predictor, "BMI"))
nrow(NHANES)
nrow(nhanes_select)
```

We need to know which ones are categorical and turn them into factors

```{r}
nhanes_select$Marijuana = as.factor(nhanes_select$Marijuana)
nhanes_select$SmokeNow = as.factor(nhanes_select$SmokeNow)
nhanes_select$HardDrugs = as.factor(nhanes_select$HardDrugs)
nhanes_select$Diabetes = as.factor(nhanes_select$Diabetes)
nhanes_select$TVHrsDay = as.factor(nhanes_select$TVHrsDay)
nhanes_select$CompHrsDay = as.factor(nhanes_select$CompHrsDay)
nhanes_select$HealthGen = as.factor(nhanes_select$HealthGen)
nhanes_select$LittleInterest = as.factor(nhanes_select$LittleInterest)
```


Now it might be handy to have a mini dataset which is devoid of NAs. We can always go back to `nhanes_select` and impute missing data. For now let's get a quick idea of what sort of model we might get. The mini dataset is `nhanes_redux`. 

```{r}
nhanes_redux = na.omit(nhanes_select)
n = nrow(nhanes_redux)
n
```
#Which variables have majority Nan values

```{r}

my_matrix = as.matrix(nhanes_select)

# Count the NA values in each column
na_counts = colSums(is.na(my_matrix))

# Calculate the percentage of NA values in each column
total_rows = nrow(my_matrix)
na_percentage = (na_counts / total_rows) * 100

# Create a data frame to store the results
na_summary = data.frame(Column = names(na_counts), NA_Count = na_counts, NA_Percentage = na_percentage)

# Print the summary
print(na_summary)


```

This is very reduced at only `r n` rows! 

## Model building

### Redux model

We can now build a first model and test for homoskedasticity, normality and equal variance. We may also get a first impression of high influence variables and the variance inflation factor. 

```{r}
fit_redux = lm(BMI ~ ., data = nhanes_redux)
summary(fit_redux)
```

Some tests

```{r}
library(lmtest)
bptest(fit_redux)
shapiro.test(resid(fit_redux))
cook_thresh = 4 * cooks.distance(fit_redux) / length(nhanes_redux)
mean(cooks.distance(fit_redux) > cook_thresh)
```

```{r, fig.asp=1}
plot(fitted(fit_redux), resid(fit_redux), col = "darkblue")
abline(h=0,col = "darkorange")
```

Fitted-residuals plot reveals no obvious deviation from homoskedasticity (arguably we are looking at a very small subset!). The p.value of the BP-test is somewhat suspicious. The Shapiro-Wilk test does not appears suspicious. 

```{r, fig.asp=1}
qqnorm(resid(fit_redux), col = "dodgerblue")
qqline(resid(fit_redux), col = "darkorange")
```

The Q-Q-Plot might show some deviations from normality. Again, subset is small. 

In the summary above many parameters had large p-values. Let's check for variance inflation. 

```{r}
library(car)
car::vif(fit_redux)
```

Some variables, such as `r names(nhanes_redux)[which(car::vif(fit_redux) > 5)]` appear to have large variance inflation. 

Let's do a quick model selection and check again. 

```{r}
fit_red_sel = step(fit_redux, direction = "backward", trace = 0)
summary(fit_red_sel)
car::vif(fit_red_sel)
```
As far as VIF is concerned this mini-model has improved over `fit_redux`. 

## Where to go from here

* The `nhanes_redux` is really tiny. We want to think about a better strategy for dealing with NAs rather than just omitting 99% of the data. 
* Once we can fit `nhanes_select_clean` we can run step-wise search again to find a reasonably good model and do the linear model tests as above again. 
* We could also then check for partial correlation for some predictors (with high VIF?). 
* ...
