---
title: "nhanes_eda"
author: "STAT 420, Summer 2023, Preeti Agrawal, Thimira Bandara, Michael Conlin, Constatin Kappel"
date: "2023-07-31"
output: html_document
---
```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
```


## Loading Data

Loading the NHANES data for exploration. 

```{r cars}
library(NHANES)
head(NHANES)
#?NHANES
```

## Ruling out variables

### Ruling out variables by reasoning or by exploratory analysis

We have chosen `BMI` (Body mass index (weight/height2 in kg/m2)) as our response variable. In NHANES, this data is reported for participants aged 2 years or older, so that will focus on participants over 2 years old for our analysis. Provided below are all the predictors in NHANES, along with our response variable `BMI`.

```{r}
sort(names(NHANES)) # alphabetic order
```

We will add all omitted predictors into an overview table `df_exclude`. The variables we would like to keep will be in `df_keep`. 

1. Some predictors can be ruled out right away. Our response variable is `BMI`, so we should not use body `Weight` or `Height` as predictors, because `BMI` is calculated by dividing the `Weight` by `Height`.  

2. The next group of predictors seems very closely related either by name or logic deduction, for example, `Age`, `AgeDecade`, `AgeMonths`. Let's quickly double-check if they are linearly related: 

```{r, fig.asp={1}}
pairs(subset(NHANES, select = c('Age', 'Age1stBaby', 'AgeDecade', 'AgeMonths', 'AgeFirstMarij', 'AgeRegMarij')))
```

`Age`, `AgeDecade` and `AgeMonth` are clearly collinear, so we only keep `Age`. Likewise, both variables for Marijuana use appear collinear, so we keep only one, say `AgeRegMarij` and may decide to drop it later if it is not useful. We might keep `Age1stBaby`. 

3. Next, let's check all variables related to alcohol:

```{r, fig.asp=1}
to_test = c("Alcohol12PlusYr", "AlcoholDay", "AlcoholYear")
pairs(subset(NHANES, select = to_test))
```

Collinearity is not as clear in this case, but we believe one predictor related to alcohol consumption may be sufficient. We will keep `AlcoholYear`. 

4. Let's now investigate the collinearity of other drug-related variables (note: `AgeRegMarij` was in the other group as well and we kept it):

```{r, fig.asp=1}
to_test = c("SmokeNow", "Smoke100", "SmokeAge", "Marijuana", "RegularMarij", "AgeRegMarij", "HardDrugs")
pairs(subset(NHANES, select = to_test))

```

Most of these predictors are categorical, so collinearity cannot be seen, except for `SmokeAge` and `AgeRegMarij`. The latter makes sense as this drug is usually consumed via smoking. We can thus use one as a proxy for the other. Let's keep `SmokeNow` and `HardDrugs` as proxies for drug abuse and its potential effect on BMI. 

5. Now let's check for collinearity between different blood pressure related variables: 

```{r, fig.asp=1}
to_test = c("BPDia1", "BPDia2", "BPDia3", "BPDiaAve", "BPSys1", "BPSys2", "BPSys3", "BPSysAve" )
pairs(subset(NHANES, select=to_test))
```

The blood pressure variables fall into two groups: diastolic and systolic blood pressure readings. We would expect there to be strongly collinearity within each group, which is the case. So, we only keep the average in each group `BPDiaAve` and `BPSysAve`.

6. Next, let's investigate a few life-style variables related to being physically active or the opposite thereof, screen time:

```{r, fig.asp=1}
to_test = c("PhysActive", "PhysActiveDays", "TVHrsDay", "CompHrsDay", "TVHrsDayChild", "CompHrsDayChild")
pairs(subset(NHANES, select=to_test))
```

Due to the nature of these variables being categorical, a clear picture of collinearity is not observable. Let's keep half of these parameters for now, which are the ones with a bit denser levels, `PhysActiveDays`, `TVHrsDay`, `CompHrsDay`. 

7. Now we should look into some other health related variables. Let's see for cholesterol and diabetes related predictors:

```{r, fig.asp=1}
to_test = c("DirectChol", "TotChol", "Diabetes", "DiabetesAge")
pairs(subset(NHANES, select = to_test))
```

`DirectChol` and `TotChol` appear to be collinear, let's keep `TotChol`. Out of the diabetes related ones, we keep `Diabetes`.

8. Now let's analyze more health related variables, such as those related to urogenesis below:

```{r, fig.asp=1}
to_test = c("UrineVol1", "UrineFlow1", "UrineVol2", "UrineFlow2")
pairs(subset(NHANES, select = to_test))
```

Urine volume and urine flow appear collinear. Moreover, there might be collinearity between the first and second urine measurement, respectively. Let's keep `UrineVol1` for now. 

9. Next up are a somewhat heterogenic group of variables related to health or mental health. For example, somebody who is depressed might show little interest in doing things. 

```{r, fig.asp=1}
to_test = c("HealthGen", "DaysPhysHlthBad", "DaysMentHlthBad", "LittleInterest", "Depressed" )
pairs(subset(NHANES, select = to_test))
```

Again, collinearity is not easy to spot in categorical variables. Let's pick `LittleInterest` as a mild form of mental health issue (which might lead to little physical activity and obesity) and `HealthGen` as a general health rating. 

10. We decided to keep `Poverty` which is a ratio of family income to poverty guidelines, and drop `HHIncomeMid` and `HHIncome`, as they both capture similar information to what the `Poverty` variable captures. 

11. Finally, let's add `nPregnancies`, `Poverty` and `SleepHrsNight` as we believe they can have an effect on BMI.  

```{r }
#Setting up the data frames with the variables we will be excluding and keeping for model building

df_exclude = data.frame(predictor = c('Weight', 'Height', 'Age1stBaby', 'AgeDecade', 'AgeMonth', 'AgeRegMarij', 'Alcohol12PlusYr', 'AlcoholDay', 'Smoke100', 'SmokeAge', 'Marijuana', 'RegularMarij', "BPDia1", "BPDia2", "BPDia3", "BPSys1", "BPSys2", "BPSys3", 'PhysActive', 'TVHrsDayChild', 'CompHrsDayChild', 'DirectChol', 'DiabetesAge', "UrineFlow1", "UrineVol2", "UrineFlow2", "DaysPhysHlthBad", "DaysMentHlthBad", "Depressed"), 
                        reason_to_omit = c('linear dependence with BMI','linear dependence with BMI', 'redundant with nPregnancies and many NAs', 'collinear with Age', 'collinear with Age', 'redundant with Marijuana', 'more sparse than AlcoholYear', 'redundant with AlcoholYear', 'redundant with SmokeNow', 'collinear with AgeRegMarij', 'redundant with AgeRegMarij, the two might be swapped', 'redundant with Marijuana', 'collinear with other blood pressure predictors', 'collinear with other blood pressure predictors', 'collinear with other blood pressure predictors', 'collinear with other blood pressure predictors', 'collinear with other blood pressure predictors', 'collinear with other blood pressure predictors', 'Redundant with PhysActiveDays', 'redundant with TVHrsDay', 'redundant with CompHrsDay', 'collinear with TotChol', 'redundant with Diabetes', 'collinear with UrineVol1', 'collinear with UrineVol1', 'collinear with UrineVol1', 'redundant with HealthGen', 'redundant with HealthGen', 'redundant with HealthGen'))

df_keep = data.frame(predictor = c('SurveyYr', 'Age', 'AlcoholYear', 'Marijuana', 'SmokeNow', 'HardDrugs', 'BPDiaAve', 'BPSysAve', 'PhysActiveDays', 'TVHrsDay', 'CompHrsDay', 'TotChol', 'Diabetes', 'UrineVol1', 'HealthGen', 'LittleInterest', 'nPregnancies',  'Poverty', 'SleepHrsNight' ))
kable(df_exclude)
```

### Selected and reduced data sets

The above selection process based on reasoning and exploration reduces the number of predictors in NHANES from `r ncol(NHANES) - 3` to `r nrow(df_keep)`. 

Let's build a dataset `nhanes_select` using just those variables. 

```{r}
nhanes_select = subset(NHANES, select =c(df_keep$predictor, "BMI"))
nrow(NHANES)
nrow(nhanes_select)
```

We need to know which ones are categorical and turn them into factors

```{r}
nhanes_select$Marijuana = as.factor(nhanes_select$Marijuana)
nhanes_select$SmokeNow = as.factor(nhanes_select$SmokeNow)
nhanes_select$HardDrugs = as.factor(nhanes_select$HardDrugs)
nhanes_select$Diabetes = as.factor(nhanes_select$Diabetes)
nhanes_select$TVHrsDay = as.factor(nhanes_select$TVHrsDay)
nhanes_select$CompHrsDay = as.factor(nhanes_select$CompHrsDay)
nhanes_select$HealthGen = as.factor(nhanes_select$HealthGen)
nhanes_select$LittleInterest = as.factor(nhanes_select$LittleInterest)
#nhanes_select$Gender = as.factor(nhanes_select$Gender)
#nhanes_select$Race1 = as.factor(nhanes_select$Race1)
#nhanes_select$Education = as.factor(nhanes_select$Education)
#nhanes_select$MaritalStatus = as.factor(nhanes_select$MaritalStatus)
#nhanes_select$SleepTrouble = as.factor(nhanes_select$SleepTrouble)

```


Now it might be helpful to have a mini dataset which is devoid of NAs (missing values). We can always go back to `nhanes_select` and impute missing data. For now let's get a quick idea of what sort of model we might get. 

Furthermore, the NHANES dataset has data for 2 survey years: 2009-10 and 2011-12. To avoid including data from the same participants twice, we will further filter our dataset down by the more recent year, 2011_12.

The mini dataset after omitting missing values and filtering by a survey year is `nhanes_redux`. 

```{r}
nhanes_redux = na.omit(nhanes_select) #removing missing values

nhanes_redux = nhanes_redux[nhanes_redux$SurveyYr == '2011_12', ] #filtering by year
nhanes_redux = subset(nhanes_redux, select = -c(SurveyYr)) #removing SurveyYr as a column for model building

#View(nhanes_redux)

n = nrow(nhanes_redux)
n
```

This is very reduced at only `r n` rows! 

### Which variables have majority Nan values

The table below is sorted according to NA percentage in descending order. 
The top 5 predictors as far as NAs are concerned are: `nPregnancies`, `SmokeNow`, `PhysActiveDays`, `Marijuana`, `HardDrugs`. When applying data imputation strategies, these five would give us the most return on the invested effort. 
```{r}
library(tidyverse)
idx_2011_12 = nhanes_select$SurveyYr=='2011_12'
my_matrix = as.matrix(nhanes_select[idx_2011_12, ])

# Count the NA values in each column
na_counts = colSums(is.na(my_matrix))

# Calculate the percentage of NA values in each column
total_rows = nrow(my_matrix)
na_percentage = (na_counts / total_rows) * 100

# Create a data frame to store the results
na_summary = data.frame(Column = names(na_counts), NA_Count = na_counts, NA_Percentage = na_percentage)
na_summary = na_summary %>%
  arrange(desc(NA_Percentage))

# Print the summary
print(na_summary)

```
### Data imputation

#### Remove NAs in `SmokeNow`

For some predictors we can impute data by utilizing information from the documentation or codebook. 
`SmokeNow` is explained as `Study participant currently smokes cigarettes regularly. Reported for participants aged 20 years or older as Yes or No, provieded they answered Yes to having somked 100 or more cigarettes in their life time. All subjects who have not smoked 100 or more cigarettes are listed as NA here.`. 
This means `Yes` stands for person has smoked > 100 cigarettes in his/her lifetime and is still a smoker. `No` means the person used to smoke (> 100 cigarettes in lifetime), but is not currently smoking. The others are listed as `NA`. We interpret this NA hence as someone who smoked < 100 cigerattes in his/her lifetime. Therefore we can introduce a new category and re-label `NA` in this columns as `Never` for someone who never smoked. 


```{r}
nhanes_select_imp = nhanes_select
nhanes_select_imp$SmokeNow = factor(nhanes_select_imp$SmokeNow, levels = c(levels(nhanes_select_imp$SmokeNow), "Never")) # New level
smokenow_is_na = is.na(nhanes_select_imp$SmokeNow) # index to all NAs
nhanes_select_imp$SmokeNow[smokenow_is_na] = 'Never' # Convert all NAs to 'Never'
sum(is.na(nhanes_select_imp$SmokeNow))
```

We successfully replace all `NA`s in `SmokeNow` by the level `Never`. This extended the number of rows without `NA` to `r nrow(na.omit(nhanes_select_imp))`. 

#### Remove NAs in `TVHrsDay` and `CompHrsDay`

The documentation says that this variable was not reported for the survey period 2009-2010. Conversely, `TVHrsDayChild` was reported during  the survey period 2011-2012 only. If we decided to merge them into one column we could drastically increase the number of data points. The implications of this shall be discussed in the last part of this report. 
With the same logic we can merge `CompHrsDay` and `CompHrsDayChild`. 

Unfortunately, to make our taks a little harder, the two survey periods also use slightly different levels.
While the period 2009-2010 used the levels `r levels(as.factor(NHANES$TVHrsDayChild))`, the other period 2011-2012 used `r levels(nhanes_select$TVHrsDay)`. 
We do the following mapping: Levels "5" and "6" from 2009-2010 map to "More_4_hr". A level between 0 and 1 hours does not exist in 2011-2012, so we map "0_to_1_hr" to "0_hrs", thus treating this level as no screen time. 

Unfortunately, the older datasets have many missing values as well:

```{r}
sum(is.na(NHANES$TVHrsDayChild[idx_2009_10]))
sum(is.na(NHANES$CompHrsDayChild[idx_2009_10]))
```

So, let's not do it. 

```{r}
#idx_2009_10 = nhanes_select$SurveyYr=="2009_10"
#idx_2011_12 = nhanes_select$SurveyYr=="2011_12"

# Merging two columns of TV consumption
#tvHrsDayChild = as.factor(NHANES$TVHrsDayChild)
#tvHrsDayChild["0"] = "0_hrs"
#tvHrsDayChild["1"] = "1_hrs"
#tvHrsDayChild[""] = "0_hrs"
#nhanes_select_imp$TVHrsDay[idx_2009_10] = NHANES$TVHrsDayChild[idx_2009_10]

# Merging two columns of computer consumption
#nhanes_select_imp$CompHrsDay[idx_2011_12] = NHANES$CompHrsDayChild[idx_2011_12]
```


```{r}
# Want to see what new model looks like with more data ??
nhanes_select = nhanes_select_imp
nhanes_redux = na.omit(nhanes_select) #removing missing values

nhanes_redux = nhanes_redux[nhanes_redux$SurveyYr == '2011_12', ] #filtering by year
nhanes_redux = subset(nhanes_redux, select = -c(SurveyYr)) #removing SurveyYr as a column for model building

#View(nhanes_redux)

n = nrow(nhanes_redux)
n
```


## Model building

### Redux model

We can now build a first linear regression model with all variables identified above, and test for homoskedasticity, normality and equal variance. We may also get a first impression of high influence variables and the variance inflation factor. 

```{r}
fit_redux = lm(BMI ~ ., data = nhanes_redux)
summary(fit_redux)
```
The p-value of the model leads to failing to reject the null hypothesis at an $\alpha$ of 0.05.

Let's do some tests on this model to identify potential issues.

```{r}
library(lmtest)
bptest(fit_redux) #to test for constant variance
shapiro.test(resid(fit_redux)) #to test for normal distribution
cook_thresh = 4 * cooks.distance(fit_redux) / length(nhanes_redux) 
mean(cooks.distance(fit_redux) > cook_thresh) #to check for influential observations
```

```{r, fig.asp=1}
#Fitted versus Residuals Plot
plot(fitted(fit_redux), resid(fit_redux), col = "darkblue", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals Plot")
abline(h=0,col = "darkorange")
```

Fitted-residuals plot reveals some deviation from homoskedasticity (arguably we are looking at a very small subset!). However, the p.value of the BP-test, to check for the constant variance assumption, is suspicious. The Shapiro-Wilk test does appear suspicious, suggesting that the normality assumption is violated. 

```{r, fig.asp=1}
#Normal Q-Q Plot
qqnorm(resid(fit_redux), col = "dodgerblue")
qqline(resid(fit_redux), col = "darkorange")
```

The Q-Q-Plot might show some deviations from normality. Again, the subset is small making it difficult to establish a pattern.  

In the summary above many parameters had large p-values. Let's check these for variance inflation. 

```{r}
library(car)
car::vif(fit_redux)
```

Some variables, such as `r names(nhanes_redux)[which(car::vif(fit_redux) > 5)]` appear to have a large variance inflation factor. 

Let's do an AIC Backward model selection and check the variance inflation factor of the selected predictors again. 

```{r}
fit_red_sel = step(fit_redux, direction = "backward", trace = 0)
summary(fit_red_sel)
car::vif(fit_red_sel) #checking the vif from the newly fitted model.
```
As far as VIF is concerned this mini-model has improved over `fit_redux` The p-value of the overall model also results in a rejection of the null hypothesis, and shows that the regression is significant. However, most of the individual predictors are not significant in this model prompting us to investigate further regarding model selection. 

Since we suspect the constant variance assumption may be violated, we next attempt a log transformation of the response as well as all two-way interactions. However, a lot of the two-way interactions gave us NA for coefficients, most likely due to the presence of categorical variables. We therefore limited the interaction to be that of Age with all the other variables except TVHrsDay which also resulted in an NA. We followed this with a backwards search with both AIC and BIC.

```{r}
#Function to calculate the LOOCVRMSE
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

#Fitting the initial model with a log transformation of the response and some interactions
fit_redux_transform_Age_int = lm(log(BMI) ~ . + Age:. - Age:TVHrsDay, data = nhanes_redux)
#summary(fit_redux_transform_Age_int)

#AIC backwards search
fit_red_sel_aic = step(fit_redux_transform_Age_int, direction = "backward", trace = 0) 
summary(fit_red_sel_aic)
calc_loocv_rmse(fit_red_sel_aic) #Calculating the LOOCV RMSE
 
#BIC backwards search
n = length(resid(fit_redux_transform_Age_int))
fit_red_sel_bic = step(fit_redux_transform_Age_int, direction = "backward", 
                            k = log(n), trace = 0)
summary(fit_red_sel_bic)
calc_loocv_rmse(fit_red_sel_bic) #Calculating the LOOCV RMSE
```
After calculating the LOOCV RMSE, we see that the model chosen using BIC performs the best. It is therefore both the best model for prediction, as it has the best LOOCV RMSE and also the best model for explanation, as it is the smallest.

## Discussion

### Wild collection of possible discussion topics t.b.d.

* Treatment of NA in `SmokeNow`. Might introduce bias for those, who didn't answer the question as someone who never smoked. 
* Merging `TVHrsDay` with `TVHrsDayChild` introduces a slight bias. Both report hours of TV consumption as per the last 30 days. While `TVHrsDayChild` stops at age 11, `TVHrsDay` does not.  

## Where to go from here

* The `nhanes_redux` is really tiny. We want to think about a better strategy for dealing with NAs rather than just omitting 99% of the data. 
* Once we can fit `nhanes_select_clean` we can run step-wise search again to find a reasonably good model and do the linear model tests as above again. 
* Note:  adding 'Gender', 'Race1',  'Education', 'MaritalStatus', 'SleepTrouble', 'Pulse' was resulting in an adjusted R-squared of 1, and loocv_rmse = Inf in the final models. If we want, we can re-try adding them or a couple of them again to see what this is happening in more detail and why.
* We could also then check for partial correlation for some predictors (with high VIF?). 
* ...
